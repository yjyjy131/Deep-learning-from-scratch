{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab-10_5_VGG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9NM8RtD8JCt35cIIVFX1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yjyjy131/Study_Deep_Learning/blob/main/Deep-Learning-Zero-To-All-PyTorch/lab_10_5_VGG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##lab-10-5 VGG"
      ],
      "metadata": {
        "id": "B7z6G_joZI0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install visdom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6reD_4aZtub",
        "outputId": "0c2adfff-ae09-426f-a491-c581e623ae40"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting visdom\n",
            "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 32.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51 kB 20.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92 kB 26.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 112 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 143 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 174 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 194 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 204 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 225 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 235 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 256 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 266 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 286 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 307 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 327 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 348 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 358 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 368 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 378 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 389 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 399 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 409 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 419 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 430 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 440 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 450 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 460 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 471 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 481 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 491 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 501 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 512 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 522 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 532 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 542 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 552 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 563 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 573 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 583 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 593 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 604 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 614 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 624 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 634 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 645 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 655 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 665 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 675 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 676 kB 26.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from visdom) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom) (2.23.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom) (22.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from visdom) (1.15.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Collecting torchfile\n",
            "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from visdom) (7.1.2)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2021.10.8)\n",
            "Building wheels for collected packages: visdom, torchfile\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=655250 sha256=781c651fb1cb01a465f7d18e7801cbe715b700ea591de54190daaf277426d495\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/d1/9b/cde923274eac9cbb6ff0d8c7c72fe30a3da9095a38fd50bbf1\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5709 sha256=189b2383519edf5c8033c6df645163c1f638808f697e5833825fe35a396c8454\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/5c/3a/a80e1c65880945c71fd833408cd1e9a8cb7e2f8f37620bb75b\n",
            "Successfully built visdom torchfile\n",
            "Installing collected packages: jsonpointer, websocket-client, torchfile, jsonpatch, visdom\n",
            "Successfully installed jsonpatch-1.32 jsonpointer-2.3 torchfile-0.1.0 visdom-0.1.8.9 websocket-client-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "aAvFdP70ZGDS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import visdom"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "mvvzUGe6Z28U"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define loss tracker"
      ],
      "metadata": {
        "id": "YqLB4K6KZ0ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./cifar10', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True, num_workers=0)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./cifar10', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=0)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_U4ZrgRZfyI",
        "outputId": "6782f8f5-0dad-44ba-95d5-d578aa0d9729"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(img) :\n",
        "  img = img / 2 + 0.5\n",
        "  npimg = img.numpy()\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "  plt.show()\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "# vis.images(images/2 + 0.5)\n",
        "# print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "metadata": {
        "id": "JVXZF75xafDq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###make VGG16"
      ],
      "metadata": {
        "id": "TgGPSSkWb2AA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models.vgg as vgg"
      ],
      "metadata": {
        "id": "_WflVQ2zb6Nf"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = [32,32,'M', 64,64,128,128,128,'M',256,256,256,512,512,512,'M'] "
      ],
      "metadata": {
        "id": "eaWkbA6yb-ox"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "\n",
        "  def __init__(self, features, num_classes=100, init_weights=True):\n",
        "    # 상속 오버로딩, 부모 클래스의 init 메소드 호출\n",
        "    super(VGG, self).__init__()\n",
        "    self.features = features\n",
        "    self.classifier = nn.Sequential(\n",
        "        # nn.Linear(in_features, out_features)\n",
        "        nn.Linear(512*4*4, 4096),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(4096, num_classes)\n",
        "    )\n",
        "    if init_weights: self._initialize_weights()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.classifier(x)\n",
        "    return x\n",
        "\n",
        "  def _initialize_weights(self) :\n",
        "    # modules() : 모델 내 정의된 layer 들을 iterable 하게 반환\n",
        "    for m in self.modules():\n",
        "      # isinstance() 모듈 내 Layer 가 어떤 형태인지 Conv2d, Liner... \n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        if m.bias is not None :\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.BatchNorm2d) :\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, 0, 0.01)\n",
        "        nn.init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "WH-Bamk-cEbf"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16= VGG(vgg.make_layers(cfg),10,True)"
      ],
      "metadata": {
        "id": "JjkB8UW0fNiA"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "ycOEu9Jkqaq9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.Tensor(1, 3, 32, 32).to(device)\n",
        "out = vgg16(a)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2P4D3hhqvTQ",
        "outputId": "46388f60-4a90-484c-d3f5-59e2c7288253"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0092,  0.0074, -0.0076, -0.0131, -0.0185,  0.0076,  0.0020, -0.0105,\n",
            "          0.0069, -0.0007]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(vgg16.parameters(), lr=0.005, momentum=0.9)\n",
        "lr_sche = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)"
      ],
      "metadata": {
        "id": "ObuZy9jyrSFy"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_plt = vis.line(Y=torch.Tensor(1).zero_(),opts=dict(title='loss_tracker', legend=['loss'], showlegend=True))"
      ],
      "metadata": {
        "id": "Y5G5J2Ddrs69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "CGM9R7Yer3_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):  \n",
        "    running_loss = 0.0\n",
        "    lr_sche.step()\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        # to(device) : GPU 를 사용해서 새로운 copy 를 만들어냄\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = vgg16(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "a14j0hOautfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "ouputs = vgg16(images.to(device))"
      ],
      "metadata": {
        "id": "37D5i_Npr3uT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.max(input, dimension)\n",
        "_, predicted = torch.max(outputs, 1)"
      ],
      "metadata": {
        "id": "9CFUeYVguDlP"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in testloader:\n",
        "    images, labels = data\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = vgg16(images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()"
      ],
      "metadata": {
        "id": "RkQO4faguSYx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}